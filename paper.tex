%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:

% preprint       Remove this option only once the paper is in final form.
%  9pt           Set paper in  9-point type (instead of default 10-point)
% 11pt           Set paper in 11-point type (instead of default 10-point).
% numbers        Produce numeric citations with natbib (instead of default author/year).
% authorversion  Prepare an author version, with appropriate copyright-space text.

\usepackage{amsmath}

%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{ifthen}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{multirow}
\usepgfplotslibrary{statistics}
\usepackage{dblfloatfix} %enable fig at bottom of page
%\input{macros.tex}

\usepackage{xcolor}
\newcommand{\todo}[1]{\color{orange}\fbox{\bfseries\sffamily\scriptsize TODO:}{\sf\small$\blacktriangleright$\textit{#1}$\blacktriangleleft$}\color{black}}
\newcommand{\sd}[1]{\color{red}\fbox{\bfseries\sffamily\scriptsize Stef:}{\sf\small$\blacktriangleright$\textit{#1}$\blacktriangleleft$}\color{black}}
\newcommand{\sk}[1]{\color{blue}\fbox{\bfseries\sffamily\scriptsize Sophie:}{\sf\small$\blacktriangleright$\textit{#1}$\blacktriangleleft$}\color{black}}
\newcommand{\cba}[1]{\color{purple}\fbox{\bfseries\sffamily\scriptsize Clement:}{\sf\small$\blacktriangleright$\textit{#1}$\blacktriangleleft$}\color{black}}
%\newcommand*{rotatebox{75}}
\input{macros}

%%%
% Legend definition (so I can change it once for all)
%%%

\def\legBase{Baseline\xspace}
\def\legSmartSyntaxPlug{Syntax\xspace}
\def\legSlangPlug{Plugin\xspace}
\def\legSlang{Slang\xspace}
\def\legRTL{Slang+RTL\xspace}
\def\LLVMMac{LLVM-Mac\xspace}
\def\GCCLinux{GCC-Linux\xspace}
\def\plotHeight{6.8cm}

%%%
% End Legend
%%%

\newcommand{\cL}{{\cal L}}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{CONF'yy}{Month d--d, 20yy, City, ST, Country}
\copyrightyear{20yy}
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm}\reprintprice{\$15.00}
\copyrightdoi{nnnnnnn.nnnnnnn}

% For compatibility with auto-generated ACM eRights management
% instructions, the following alternate commands are also supported.
%\CopyrightYear{2016}
%\conferenceinfo{CONF'yy,}{Month d--d, 20yy, City, ST, Country}
%\isbn{978-1-nnnn-nnnn-n/yy/mm}\acmPrice{\$15.00}
%\doi{http://dx.doi.org/10.1145/nnnnnnn.nnnnnnn}

% Uncomment the publication rights used.
%\setcopyright{acmcopyright}
\setcopyright{acmlicensed}  % default
%\setcopyright{rightsretained}

%\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{Assessing primitives performance on multi-stage execution}   % 'preprint' option specified.

\title{Assessing primitives performance\\ on multi-stage execution \todo{modify maybe}}
%\subtitle{Subtitle Text, if any\titlenote{with optional subtitle note}}

\authorinfo{Sophie Kaleba}
           {RMoD - Inria Lille}
           %{Lille, France}
           {sophie.kaleba@etudiant.univ-lille1.fr} 
           
\authorinfo{Cl\'ement B\'era}
           {Software Languages Lab - Vrije Universiteit Brussel}
           {clement.bera@vub.ac.be}
           
\authorinfo{{St\'ephane Ducasse}}
           {RMoD - Inria Lille}
           {stepahne.ducasse@inria.fr}

\maketitle

Virtual machines, besides the interpreter and just-in-time compiler optimization facilities, also include a set of primitive operations that the client language can use. Some of these are essential and cannot be performed in any other way. Others are optional: they can be expressed in the client language but are often implemented in the virtual machine to improve performance when the just-in-time compiler is unable to do so (start-up performance, speculative optimizations not implemented or not mature enough, etc.).

In a hybrid runtime, where code is executed by an interpreter and a just-in-time compiler, the implementor can choose to implement optional primitives in the client language, in the virtual machine implementation language (typically C or C++), or on top of the just-in-time compiler back-end. This raises the question of the maintenance and performance trade-offs of the different alternatives. As a case study, we implemented the String comparison optional primitive in each case. This paper describes the different implementations, discusses the maintenance cost of each of them and evaluates for different string sizes the execution time in Cog, a Smalltalk virtual machine.
\end{abstract}

% 2012 ACM Computing Classification System (CSS) concepts
% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
%\begin{CCSXML}
%<ccs2012>
%<concept>
%<concept_id>10011007.10011006.10011008</concept_id>
%<concept_desc>Software and its engineering~General programming languages</concept_desc>
%<concept_significance>500</concept_significance>
%</concept>
%<concept>
%<concept_id>10003752.10010124.10010138.10010143</concept_id>
%<concept_desc>Theory of computation~Program analysis</concept_desc>
%<concept_significance>300</concept_significance>
%</concept>
%</ccs2012>
%\end{CCSXML}
%
%\ccsdesc[500]{Software and its engineering~General programming languages}
%\ccsdesc[300]{Theory of computation~Program analysis}
% end generated code

% Legacy 1998 ACM Computing Classification System categories are also
% supported, but not recommended.
%\category{CR-number}{subcategory}{third-level}[fourth-level]
%\category{D.3.0}{Programming Languages}{General}
%\category{F.3.2}{Logics and Meanings of Programs}{Semantics of Programming Languages}[Program analysis]

\keywords
Just-in-Time compiler, Primitive, Virtual machine, Managed runtime

\section{Introduction}
\label{sec:intro}

High-level object-oriented programming languages are often implemented on top of a virtual machine (VM). As VMs have become more popular, different techniques have been set up using Just-In-Time (JIT) compilation to improve the overall runtime performance: method JITs \cite{ursPHD} which usually compile methods to native code and tracing JITs \cite{Dynamo, PyPyTracing} which usually compile linear traces of execution into native code.

On top of its virtual machine, the client language can use a set of \emph{primitives}\footnote{We use in the paper the Smalltalk terminology (primitive) as this is the programming language used for our evaluation. Some other programming languages, such as Javascript, prefer to use the term built-in instead of primitive.}, which are performed directly by the interpreter rather than by evaluating expressions in a method. We distinguish two kinds of primitives: \todo{motivate the distinction. See the algorithmic primitives paper. Is there a reference stating the different types of primitives ? }
\begin{itemize}
	\item \emph{Essential primitives} cannot be performed in any other way. A high-level object-oriented language without primitives can move values from one variable to another, but cannot add two integers together. Many arithmetic and comparison operations between numbers are primitives. Some primitives allow one to communicate with I/O devices such as the disk, the display and the keyboard.
	\item \emph{Optional primitives} exist only to make the system run faster \cite{blueBook}. They can be implemented in the client language directly, making its implementation as a primitive optional, but they are often implemented directly in the VM to improve performance.
\end{itemize}


\paragraph{Optional primitives as client code.}
The implementation of an optional primitive in the VM rather than the client language improves performance but also often increases the implementation engineering cost. For example, accessing objects in the VM usually requires understanding of the memory layout or implementation details, which are not needed in the client language.

For this reason, some virtual machine implementors attempted to remove most or all optional primitives. Their goal was to use a JIT performing speculative optimizations to be able to implement the optional primitives in the client language without performance loss. Ideally they wanted to get   the performance of the same primitives written in the VM. Self \cite{ursPHD} and Strongtalk \cite{Strongtalk} were the first to try this approach. Early versions of the Javascript engine V8 \cite{V8} also implemented most of the primitives, including for example Array operations in Javascript itself. However, without optional primitives implemented in the VM, several problems rise:
\begin{itemize}
	\item High performance relies on a JIT with speculative optimizations, which is hard to implement, maintain and evolve. \todo{a ref would be nice}
	\item There is a big performance gap between the baseline performance and the peak performance. \todo{a ref would be nice}
	\item Even mature JITs fail to optimize some narrow cases, where the performance is drastically slower \\~\cite{VMHotCold}.
\end{itemize}

Overall, this approach requires high engineering time to get a mature optimizing JIT with speculative optimizations and even then, it leads in practice to unreliable performance.

\paragraph{World border cost.}
VMs are traditionally implemented in a low-level language such as C or C++. To balance between memory footprint, start-up performance and peak performance, the execution of code is usually done through multiple execution tiers: the first few executions of a code snippet are done through an interpreter and the JIT compiler optimizes at runtime the frequently used code snippets.

This leads to annoying concerns: for example, let's say we implement a primitive in C. Frequently used portion of code calling that primitive are going to be compiled to native code by the JIT. This is problematic since the native code generated by the JIT is not directly compatible with native code generated by the C compiler: a call from one to another may require to edit the stack pointer and the frame pointer from the client stack to the C stack and to spill or move multiple registers. Switching between both usually wastes up to around a dozen native instructions. If the primitive is going to execute many instructions, the switch overhead might be negligible, but if the primitive executes only a few instructions, the overhead can be noticeable.

\paragraph{Measurements.}
In this paper, we want to measure the complexities and gain of different implementations. For this purpose, we take the example of a string comparison optional primitive. The primitive compares two strings and answers if one string is greater, equal or lower than the other string. The exact specification is detailed in Section \ref{subsec:primSpec}. The primitive is convenient for measurements since it can be executed with strings of small sizes or large sizes, leading the primitive to execute many processor instructions or only a few of them. 

We implemented a first version of the optional primitive in Pharo. We then implemented two versions compiling through C compilers to native code, using different parts of our VM infrastructure. Lastly, we implemented a version in the back-end of the JIT. Each time, we tried to write the code in the most efficient way possible. Then, we compared the execution time of all versions on strings of different sizes, showing that overall the most complex implementation is the fastest to execute.

Section~\ref{sec:implContext} describes our implementation context, \emph{i.e.,} the execution model of the VM we used for our evaluation, how it executes primitives and the specification of our string comparison primitive. Section~\ref{sec:implem} details the various implementations of the primitive we used for the evaluation. In Section~\ref{sec:eval}, we evaluate the performance of the different implementations with different C compilers and string sizes. We also show the native code of the performance critical part of the primitive generated by each version and discuss it. Further sections compare our work to related works, discuss future work and conclude.



\section{Implementation context}
\label{sec:implContext}

Our evaluation is based on the Cog VM: a Smalltalk virtual machine. The Cog VM is based on Dan Ingalls' Smalltalk VM \cite{BackToTheFuture} and has been enhanced with a JIT \cite{CogJIT} compiling one method or closure at a time to native code. Historically, the VM implemented a 32-bits adaptation of the Smalltalk-80 specifications \cite{blueBook}, but the Cog VM is now the default VM for multiple programming languages such as Pharo \cite{PharoByExample}, Squeak \cite{SqueakByExample} and Newspeak \cite{NewspeakOopsla}.

Section~\ref{sec:VMCompil} describes briefly %\sd{no it does not describe that: the implementation languages and}
the compilation process of the Cog VM. Section~\ref{sec:VMexec} explains briefly the hybrid execution model with an interpreter and a JIT. In Section~ \ref{sec:primExec}, we detail how primitives are executed. Section~\ref{subsec:primSpec} discusses the string comparison primitive specifications  used for our evaluation.

\subsection{VM compilation}
\label{sec:VMCompil}

\begin{figure}[bth!]
		\centering
		\includegraphics[width=0.62\linewidth]{figures/VMCompilation}
		\caption{Cog VM compilation}
		\label{fig:VMCompilation}
\end{figure}

Most of the Cog VM code base is written in Slang, a subset of Smalltalk. Slang is compiled to C and then to native code through standard C compilers. The execution engine (the memory manager, the interpreter and the baseline JIT) is entirely written in Slang. The two main purposes of using Slang code over plain C are:
\begin{itemize}
	\item to specify with annotations what function needs to be inlined or duplicated with constant parameters, and
	\item to be able to simulate VM execution, executing the Slang code as Smalltalk code on top of a compiled VM for debugging purposes.
\end{itemize}

The executable is generated in two steps as shown on Figure~\ref{fig:VMCompilation}, similarly to the RPython toolchain \cite{RPythonToolchain}. The first step is to generate the two C files representing the whole execution engine written in Slang using the Slang-to-C compiler. During the second step, a C compiler is called to compile the execution engine and the platform-specific code written directly in C to the executable VM.

\paragraph{Plugins.}The Cog VM can be extended using plugins \cite{Guzd01b}. VM plugins enable the addition of new features to the VM. The main benefit of writing a plugin over extending the VM is modularity, plugins are in separate code bases and can be easily added or removed from the VM. Plugins can be compiled as internal or external:
\begin{itemize}
	\item \emph{External plugins} are compiled as a dynamic libraries distributed with the VM. They can be added or removed from a compiled VM: each dynamic library can be removed, or recompiled separately and modified.
	\item \emph{Internal plugins} are compiled with the VM executable. They can be added or removed at VM compilation time.
\end{itemize}

There is a performance overhead for the plugins compared to normal VM code. For example, accesses to object headers are dependent on the memory representation and therefore need to be done indirectly through calls to main VM executable, since the plugin cannot know ahead of time which memory representation is used. Such calls cannot be inlined by the C compiler\footnote{In the case of specific internal plugins, C compiler linking time optimizations can be enabled to limit this overhead.}. 

\subsection{VM execution}
\label{sec:VMexec}

The Cog VM executes the client's code using an hybrid runtime with an interpreter and a JIT. The interpreter uses a global look-up cache to speed-up virtual calls. On a cache hit, it requests the JIT to compile that method and the native code version is used instead. In most case, the JIT is used at the second activation of the method. Closures have similar heuristics.

The VM code, including the interpreter code, is compiled by a C compiler and uses the C stack at runtime. During its execution, the interpreter modifies the execution stack of the client language, which is different from its own C execution stack as shown on Figure~\ref{fig:TwoStacks}. This means for example that the frame pointer register in the processor refers to a stack frame on the C stack and that a processor push instruction pushes a value on the C stack. When executing the native code generated by JIT, only the client language stack is used. This means for example that the frame pointer register in the processor refers to a stack frame on the client stack and that a processor push instruction pushes a value on the client stack.

\begin{figure}[tbh]
		\centering
		\includegraphics[width=0.96\linewidth]{figures/TwoStacks}
		\caption{Client and C stacks.}
		\label{fig:TwoStacks}
\end{figure}

Because of this design, switching the execution between the interpreter and the native code generated by the JIT has a cost. Effectively, the VM needs to switch the frame pointer, the stack pointer and the link register (the latter only on some architectures) from one stack to the other one. It also needs to save those pointers to be able to resume execution from one stack to the other one. In addition, caller-saved registers on the C conventions or the JIT convention need to be saved since the VM cannot tell ahead what code is going to be used in the other execution model and what registers are required. We estimate the overhead to up to around a dozen of processor instructions, the exact number of numbers varies a lot (the number of registers to edit depends on the processor used and on the code executed).

Most of the primitives are implemented either in Slang or directly in C. They benefit from the C compiler optimizations to be efficient. When called from the interpreter, activating a primitive is just a C function call (the primitive function pointer is cached in the look-up cache next to the method to activate). When called from the native code generated by the JIT, activating a primitive requires to switch the execution from the client stack to the C stack, perform the primitive, and switch back to resume execution. If the primitive takes a significant amount of instructions to execute, for example, the primitive is copying 100kb from one array to another one, the stack switch dance overhead is negligible. If the primitive is very quick to perform, for example, the primitive is adding two integers which do not overflow, the overhead is massive: the execution cost of the addition goes from a few processor instructions to a few dozen instructions.

To solve this performance bottleneck, the Cog VM allows one to redefine primitives in Cog's Register Transfer Language (RTL), an abstract assembly which is compiled to native code through Cog's JIT back-end. Such primitives are then generated to native code at runtime when a method annotated with the primitive is compiled to native code. This allows the primitive to be performed in the client stack with the client calling convention, saving register moves and the stack switch dance. Primitives redefined in such way can be entirely or partially redefined. In the latter case, only the common cases are re-implemented on top of the JIT back-end. If an unimplemented case happens at runtime, the generated native code calls the C primitive instead, adding overhead only in uncommon cases.

\subsection{Primitive execution}
\label{sec:primExec}

In the Cog VM, primitives are always associated with compiled methods. A compiled method has information in its header to inform the virtual machine if it has a primitive operation or not. When activating a method with a primitive, the primitive function is executed before the method's bytecode. If the primitive succeeds, the primitive returns a result, as if it were the result of the virtual call. If the primitive fails, it does so without side-effects and execution continues executing the method's bytecode, as if the primitive had not been present.

To fail without side-effects a primitive must validate any parameters and any state fetched from them, before changing execution state by performing its operations. Validation involves any of testing for a specific class, testing for bit vs pointer objects, bounds checks, and recursively applying these tests to substructure of the parameters. For example, the primitive that installs a cursor examines the first parameter to check that it represents a valid cursor object, comprised of two bitmaps, one for the image and one for the shape, plus a point to specify the cursor's hotspot.

%The primitives can be associated either with a name, either with an index in the Cog VM. The named primitives can be added and modified without the need to recompile the VM.

\subsection{String comparison primitive}
\label{subsec:primSpec}
In this section we describe first the string representations then the string comparison primitive specification.

\paragraph{String representations.} Strings are represented on top of the Cog VM in two possible forms: ByteStrings and WideStrings. ByteStrings encode each character of the string in a single byte. The encoding usually follows the extended ASCII standard. \todo{we should not use this term} WideStrings encode each character in 32 bits. The encoding usually follows Unicode specifications. In this paper, we will discuss only the implementation of the string comparison primitive for ByteStrings. WideStrings use an implementation entirely implemented in the client language, and so far, it has not been reported as a performance bottleneck for any program of any users. To sort arrays of strings, strings can be compared. They can be compared in the default order (ASCII or Unicode) or using different orders, for example, the case insensitive order.

\paragraph{Primitive specification.} The String comparison optional primitive takes two or three parameters. The two first operands are the two ByteStrings to compare and the third operand, optional, is the order table (a ByteArray of size 256 encoding the order of characters). If no order is specified, the VM assumes the byte ordering (ASCII order) is the order to compare against, and compares directly the bytes of the ByteStrings. If an order is specified, for example a case insensitive order (the entries for the 41 and 61 in the order byte array, respectively characters A and a in the ASCII table, both answer the same value), the primitive compares the bytes through the indirection order. Having the order optional allows to have quicker string comparison in the common case where the ASCII order is used. The primitive answers a negative integer, zero, or a positive integer as the first parameter is less than, equal to, or greater than the second parameter.

In the paper we will focus on the comparison of two strings with the default ASCII ordering and measure the performance only of this case. The goal of this work was to improve the performance of common string operations, typically string equality which internally uses the primitive, and the cases where the order is non ASCII are considered uncommon.

Conceptually, the primitive with two parameters first checks if the operands are ByteStrings and fails if they are not. It then extract the ByteString sizes from their object header and computes the minimum size. Lastly, it iterates over the two ByteStrings until the minimum size is reached. If a difference is found, if answers the difference between the two different characters. If no difference is found, it answers the difference between the two string sizes.

\section{Different primitive implementations}
\label{sec:implem}

This section describes the different implementations of the string comparison optional primitive and then compare them.

We then explain how each of these versions is executed by the VM and the pros and cons of each and every.

 \subsection{Different implementations}
 
We distinguish four implementations: the \legBase in pure Smalltalk, the \legSlangPlug version, the \legSlang version and the \legRTL version.

\paragraph{\legBase.}
To evaluate the primitive performance, we use as baseline an implementation in pure Smalltalk. There are multiple ways of writing the comparison of two strings in Smalltalk, following Smalltalk coding conventions, a standard developer would likely use high-level constructs which apply a closure on each element of the strings. We did not use high-level construct and chose to implement the Smalltalk version the most optimized way Smalltalk allows, \emph{i.e.,} we used the constructs to:do: and ifFalse:, which are both recognized by the Smalltalk to bytecode compiler and compiled respectively to loops and branches at the bytecode level\footnote{The bytecode compiler moves the upper limit of the loop ahead of the loop, so (length1 min: length2) is not compute at each iteration.}. The implementation follows the new specification, hence two methods are available, baselineCompareWith: and baselineCompareWith:order:. Since the focus is only on ASCII ordered comparison, we show the code only of baselineCompareWith:.

\begin{code}
baselineCompareWith: aString
	| c1 c2 length1 length2 |
	length1 := self size.
	length2 := aString size.
	1 to: (length1 min: length2) do: [ :i |
		(c1 := self basicAt: i) = (c2 := aString basicAt: i) ifFalse: [ ^ c1 - c2 ] ].
	^ length1 - length2
\end{code}

%\paragraph{Existing: Smart Syntax.}
%
%The Smart Syntax Plugin is one of the version \sd{of what} existing until we started this work. The idea behind Smart Syntax is the following: the programmer writes code in Smalltalk using exclusively constructs that are recognized by the Smart Syntax compiler and that can be directly compiled to C (\emph{e.g.}, computing the size of a byte array, accessing its fields and doing integer arithmetic). With the addition of type annotations in the form of pragmas \cite{Duca16a} \footnote{Pragma are method annotation.}, the Smart Syntax plugin is able to generate C code from the Smalltalk code. 
%With this approach, the programmer has to write the optional primitive code a single time in Smalltalk. This code is used both as the fall-back code if the VM does not implement the primitive and at Slang to C compilation time to generate a C plugin from the code and the type annotations. This primitive follows the old specification, the full code is available in Appendix~\ref{SmartSyntaxAppendix}.

\paragraph{\legSlangPlug.}
Written in Slang, the VM plugin implementation is compiled to C independently to the VM and is then compiled to native code through the C compiler as an internal plugin, without enabling linking time optimizations. Accesses to the ByteString object header (to check if they are actually ByteString and not other objects as well as extracting their size), and accesses to the stack (to read the operands) are done indirectly through the main VM API since they're dependent on the memory representation of objects and stack layout used. These calls generate some overhead since they cannot be inlined. To avoid such calls in the main comparison loop, the primitive requests the main VM to provide a pointer to the first byte of each string and then assumes all bytes are then contiguous.

\paragraph{\legSlang.}
The Slang implementation is written inside the interpreter and has access to the full interpreter APIs. It is compiled to C and then to native code as part of the main VM. There is no overhead in accessing the object memory representation or the stack as in the \legSlangPlug version.

\paragraph{\legRTL.} 
In addition to the previous implementation, we've re-implemented here the primitive on top of Cog's JIT back-end. The primitive is written in Cog's RTL, an abstract assembly whose instructions compile almost one-to-one to native instructions. This primitive requires a more important amount of work to be written: we had to be careful to generate efficient machine code (there is no C compiler to optimize the code) and debugging such code requires the use of a processor simulator such as Bochs. Since only performance critical implementations matter in this context, we've only re-implemented the case where there are two parameters. For the three parameter primitive, the VM falls back to the Slang version. 

\subsection{Comparison}
\label{sec:implem_cmp}

\begin{table*} [bth]
\centering
\begin{tabular}{c|cccc|ccc}
   &  \multicolumn{4}{c|}{Software maintenance \& evolution} & \multicolumn{3}{c}{Primitive execution time}\\
  \hline
    & Recompilation & Familiar to & Lines & Number of & Stack switch & Optimization & Code generation \\
    & required & developers & of code & implementations & overhead & potential & control \\
  \hline
  \legBase    & No & All  & 10  & 0 & No & - & - \\
   \hline
  \legSlangPlug & Plugin & VM & 35  & 1  & Yes & Limited & Low \\
   \hline
  \legSlang   & Yes  & VM & 35  & 1  & Yes & High & Low \\
   \hline
  \legRTL   & Yes  & VM-JIT & 85  & 2  & No & High & High \\
  \hline
\end{tabular}
\caption{Comparison between implementations.\vspace{-0.5cm}}
\label{tab:cmp_implem}
\end{table*}

 In this section we compare the four different implementations according to the following criteria:
% \begin{itemize}
% 	\item \emph{Recompilation required:} Changing the implementation requires only Smalltalk bytecode recompilation (No), requires to recompile a plugin to a dynamic library if compiled as external or the recompilation of the full VM if compiled as internal (Plugin) or requires to recompile the full VM (Yes). 
%	\item \emph{Language familiar to developers:} Smalltalk developers are considered to be familiar with Smalltalk only, while Cog VM developers are considered being familiar with Smalltalk, Slang, C and Assembly code. Only Cog's JIT implementors are familiar with its RTL, which is a subset of the VM developers.  We use All if all communities are familiar with the language, VM if all VM developers are and VM-JIT if only a subset of VM developers are.
%	\item \emph{Number of lines of code:} We measured the number of lines of code of each implementation, which arguably can be used to evaluate the complexity level of each implementation.
%	\item \emph{Number of implementations to maintain:} In each case, the client language has to implement a fall-back code in Smalltalk to be executed if the primitive fails. This code is maintained by the client language implementors, and not by the VM implementors. Hence we do not count the fall-back code in the number of implementation to maintain, nor the pure Smalltalk implementation. However, as a VM implementor, we need to maintain what generates the C code of the VM. We count all the Slang implementations as well as the ones on Cog's RTL.
%	\item \emph{Stack switch overhead:} Yes if activating the primitive from the native code generated by the JIT requires to switch between the C and the client stack, inducing execution time overhead.
%	\item \emph{Optimization potential:}
%	C cross-file inlining is not performed by default for internal plugins. External plugins don't have access to the whole VM API and have to go through an interface to do so, leading to an overhead, limiting the performance unless the string size is really high.
%	\item \emph{Control over the generated code:} The implementations written in Slang are compiled by the C compilers into native code. The generated code can therefore differ according the compilers and the optimizations they performed. On the contrary, the JIT generates the same native code whichever C compiler is used.
%\end{itemize}

%Vspce to fit all on page.
\vspace{-0.1cm}
	\paragraph{Recompilation required:} Changing the implementation requires only Smalltalk bytecode recompilation (No), requires to recompile a plugin to a dynamic library if compiled as external or the recompilation of the full VM if compiled as internal (Plugin) or requires to recompile the full VM (Yes). 
\vspace{-0.1cm}
	\paragraph{Language familiar to developers:} Smalltalk developers are considered to be familiar with Smalltalk only, while Cog VM developers are considered being familiar with Smalltalk, Slang, C and Assembly code. Only Cog's JIT implementors are familiar with its RTL, which is a subset of the VM developers.  We use All if all communities are familiar with the language, VM if all VM developers are and VM-JIT if only a subset of VM developers are.
\vspace{-0.1cm}
	\paragraph{Number of lines of code:} We measured the number of lines of code of each implementation, which arguably can be used to evaluate the complexity level of each implementation.
\vspace{-0.1cm}
	\paragraph{Number of implementations to maintain:} In each case, the client language has to implement a fall-back code in Smalltalk to be executed if the primitive fails. This code is maintained by the client language implementors, and not by the VM implementors. Hence we do not count the fall-back code in the number of implementation to maintain, nor the pure Smalltalk implementation. However, as a VM implementor, we count all the Slang implementations as well as the Cog's RTL one.
\vspace{-0.1cm}
	\paragraph{Stack switch overhead:} Yes if activating the primitive from the native code generated by the JIT requires to switch between the C and the client stack, inducing execution time overhead.
\vspace{-0.1cm}
	\paragraph{Optimization potential:}
	C cross-file inlining is not performed by default for internal plugins. External plugins don't have access to the whole VM API and have to go through an interface to do so, leading to an overhead, limiting the performance unless the string size is really high.
\vspace{-0.1cm}
	\paragraph{Control over the generated code:} The implementations written in Slang are compiled by the C compilers into native code. The generated code can therefore differ according the compilers and the optimizations they performed. On the contrary, the JIT generates the same native code whichever C compiler is used.
\vspace{0.2cm}


Table \ref{tab:cmp_implem} summarizes the main pros and cons related to each implementation: the engineering time tends to increase when more low-level languages are used. VM recompilation slows slightly the development process, but the resulting code is usually faster to run. Likewise, the use of plugins brings modularity at the cost of some performance loss.

%\vfill\null %Title not at bottom of page please

\section{Evaluation}
\label{sec:eval}

\todo{mention baseline here}
The \legSlangPlug, \legSlang and \legRTL implementations of this string comparison primitive have been implemented in the Cog VM. In this section, we compare execution speed of these primitives against the other existing implementations. We measure their respective execution times for different string sizes.% \sd{mention microbenchmarks here.}\sd{why there is only 4.3 and not something else? Because this is strange from a shape structure.}

\subsection{Set-up}

To evaluate the performance, we had to compile a special VM with all the different primitive implementations included. The production VM normally only features one string comparison primitive implementation, not three of them. We compiled one VM for Linux with GCC, and one for Mac with LLVM (exact version numbers below). We ran all benchmarks on the same run of the same VM on both platforms.

\paragraph{\GCCLinux.}The \GCCLinux evaluation was performed on an Asus Zenbook with Ubuntu 16.04.4 LTS, a 2.2 Ghz processor Intel Core 5, 8 Gb 1600MHz DDR3 of RAM. The Linux VM was compiled using GCC version 5.4.0.

\paragraph{\LLVMMac.}The \LLVMMac evaluation was performed on a MacBook pro with Mac OS 10.11.6, a 2.9 Ghz processor Intel Core 5, 8 Gb 1867MHz DDR3 of RAM. The Mac VM was compiled using Apple LLVM version 8.0.0 (clang-800.0.38).

\paragraph{}The following evaluation was performed on 32 bits Intel VMs (x86).

\begin{figure*}[htb!]
  \centering
    \begin{subfigure}[b]{\textwidth}
    \begin{subfigure}[b]{.19\textwidth}
    \subcaption*{Size 0}
    \begin{tikzpicture}
      \begin{axis}
      [
        height=\plotHeight,
        x=0.5cm,
        xtick={1,...,4},
        xticklabel style={rotate=90},
        bar width=0.3cm,
        enlarge x limits={abs=0.45cm},
        xticklabels={\legBase, \legSlangPlug, \legSlang, \legRTL},
        font={\footnotesize},
	ybar=10pt,
	ymin=0
        ]
\addplot[
    fill=black!25,
    draw=black,
    fill,
    point meta=y,
    every node near coord/.style={inner ysep=5pt},
    error bars/.cd,
        y dir=both,
        y explicit
]
table [y error=error] {
x   y           error
1   1           0
2   0.413386   0.0004942463
3   1.286553    0.004837948
4   3.343083    0.02105667
};
      \end{axis}
    \end{tikzpicture}
    \end{subfigure}
     \begin{subfigure}[b]{.19\textwidth}
    \subcaption*{Size 1}
    \begin{tikzpicture}
      \begin{axis}
      [
        height=\plotHeight,
        x=0.5cm,
        xtick={1,...,4},
        xticklabel style={rotate=90},
        bar width=0.3cm,
        enlarge x limits={abs=0.45cm},
        xticklabels={\legBase, \legSlangPlug, \legSlang, \legRTL},
        font={\footnotesize},
	ybar=10pt,
	ymin=0
        ]
\addplot[
    fill=black!25,
    %draw=none,
    %fill,
    draw=black,
    fill,
    point meta=y,
    every node near coord/.style={inner ysep=5pt},
    error bars/.cd,
        y dir=both,
        y explicit
]
table [y error=error] {
x   y           error
1   1           0
2   0.745517   0.0008472452
3   2.256624    0.011554253
4   5.171014    0.02650453
};
      \end{axis}
    \end{tikzpicture}
    \end{subfigure}
    \begin{subfigure}[b]{.19\textwidth}
    \subcaption*{Size 5}
    \begin{tikzpicture}
      \begin{axis}
      [
        height=\plotHeight,
        x=0.5cm,
        xtick={1,...,4},
        xticklabel style={rotate=90},
        bar width=0.3cm,
        enlarge x limits={abs=0.45cm},
        xticklabels={\legBase, \legSlangPlug, \legSlang, \legRTL},
        font={\footnotesize},
	ybar=10pt,
	ymin=0
        ]
\addplot[
    fill=black!25,
    draw=black,
    point meta=y,
    every node near coord/.style={inner ysep=5pt},
    error bars/.cd,
        y dir=both,
        y explicit
]
table [y error=error] {
x   y           error
1   1           0
2   1.804201   0.0004602098
3   4.958408   0.011106844
4   8.714720   0.02207683
};
      \end{axis}
    \end{tikzpicture}
    \end{subfigure}
    \begin{subfigure}[b]{.19\textwidth}
    \subcaption*{\hspace{0.4cm}Size 100}
 \begin{tikzpicture}
      \begin{axis}
      [
        height=\plotHeight,
        x=0.5cm,
        xtick={1,...,4},
        xticklabel style={rotate=90},
        bar width=0.3cm,
        enlarge x limits={abs=0.45cm},
        xticklabels={\legBase, \legSlangPlug, \legSlang, \legRTL},
        font={\footnotesize},
	ybar=10pt,
	ymin=0
        ]
\addplot[
    fill=black!25,
    draw=black,
    point meta=y,
    every node near coord/.style={inner ysep=5pt},
    error bars/.cd,
        y dir=both,
        y explicit
]
table [y error=error] {
x   y           error
1   1           0
2   12.047182   0.0088056909
3   17.240628   0.051498584
4   17.656653   0.04813202
};
      \end{axis}
    \end{tikzpicture}
    \end{subfigure}
    \begin{subfigure}[b]{.19\textwidth}
    \subcaption*{\hspace{0.5cm}Size 1000}
     \begin{tikzpicture}
      \begin{axis}
      [
        height=\plotHeight,
        x=0.5cm,
        xtick={1,...,4},
        xticklabel style={rotate=90},
        bar width=0.3cm,
        enlarge x limits={abs=0.45cm},
        xticklabels={\legBase, \legSlangPlug, \legSlang, \legRTL},
        font={\footnotesize},
	ybar=10pt,
	ymin = 0
        ]
\addplot[
    fill=black!25,
    draw=black,
    point meta=y,
    every node near coord/.style={inner ysep=5pt},
    error bars/.cd,
        y dir=both,
        y explicit
]
table [y error=error] {
x   y           error
1   1           0
2   25.35945   0.1742098
3   25.42575   0.1621165
4   26.76261   0.186075
};
      \end{axis}
    \end{tikzpicture}
   \end{subfigure}
  % \caption{GCC-Linux results}
   % \label{fig:gcc}
   \subcaption*{\textbf{\GCCLinux}\vspace{0.25cm}}
   \label{fig:gcc}
   \end{subfigure}
 \begin{subfigure}[b]{\textwidth}
    \begin{subfigure}[b]{.19\textwidth}

    \begin{tikzpicture}
      \begin{axis}
      [
        height=\plotHeight,
        x=0.5cm,
        xtick={1,...,4},
        xticklabel style={rotate=90},
        bar width=0.3cm,
        enlarge x limits={abs=0.45cm},
        xticklabels={\legBase, \legSlangPlug, \legSlang, \legRTL},
        font={\footnotesize},
	ybar=10pt,
	ymin=0,
	]
\addplot[
    fill=black!25,
    draw=black,
    point meta=y,
    every node near coord/.style={inner ysep=5pt},
    error bars/.cd,
        y dir=both,
        y explicit
]
table [y error=error] {
x   y           error
1   1           0
2   0.3391346   0.0006874275
3   0.9324989   0.001469945
4   3.254642   0.007910739
};
      \end{axis}
    \end{tikzpicture}

    \end{subfigure}
     \begin{subfigure}[b]{.19\textwidth}

    \begin{tikzpicture}
      \begin{axis}
      [
        height=\plotHeight,
        x=0.5cm,
        xtick={1,...,4},
        xticklabel style={rotate=90},
        bar width=0.3cm,
        enlarge x limits={abs=0.45cm},
        xticklabels={\legBase, \legSlangPlug, \legSlang, \legRTL},
        font={\footnotesize},
	ybar=10pt,
	ymin=0
        ]
\addplot[
    fill=black!25,
    draw=black,
    point meta=y,
    every node near coord/.style={inner ysep=5pt},
    error bars/.cd,
        y dir=both,
        y explicit
]
table [y error=error] {
x   y           error
1   1           0
2   0.6237533   0.0012023524
3   1.6900368   0.003040773
4   5.215819   0.015275079
};
      \end{axis}
    \end{tikzpicture}

    \end{subfigure}
    \begin{subfigure}[b]{.19\textwidth}

    \begin{tikzpicture}
      \begin{axis}
      [
        height=\plotHeight,
        x=0.5cm,
        xtick={1,...,4},
        xticklabel style={rotate=90},
        bar width=0.3cm,
        enlarge x limits={abs=0.45cm},
        xticklabels={\legBase, \legSlangPlug, \legSlang, \legRTL},
        font={\footnotesize},
	ybar=10pt,
	ymin=0
        ]
\addplot[
    fill=black!25,
    draw=black,
    point meta=y,
    every node near coord/.style={inner ysep=5pt},
    error bars/.cd,
        y dir=both,
        y explicit
]
table [y error=error] {
x   y           error
1   1           0
2   1.5807038   0.0092529942
3   3.2413871   0.006737660
4   8.970703   0.035921295
};
      \end{axis}
    \end{tikzpicture}
    \end{subfigure}
    \begin{subfigure}[b]{.19\textwidth}
 \begin{tikzpicture}
      \begin{axis}
      [
        height=\plotHeight,
        x=0.5cm,
        xtick={1,...,4},
        xticklabel style={rotate=90},
        bar width=0.3cm,
        enlarge x limits={abs=0.45cm},
        xticklabels={\legBase, \legSlangPlug, \legSlang, \legRTL},
        font={\footnotesize},
	ybar=10pt,
	ymin=0
        ]
\addplot[
    fill=black!25,
    draw=black,
    point meta=y,
    every node near coord/.style={inner ysep=5pt},
    error bars/.cd,
        y dir=both,
        y explicit
]
table [y error=error] {
x   y           error
1   1           0
2   11.4096285   0.0473858056
3   10.1485203   0.032077214
4   17.642998  0.059953679
};
      \end{axis}
    \end{tikzpicture}
    \end{subfigure}
    \begin{subfigure}[b]{.19\textwidth}

    \begin{tikzpicture}
      \begin{axis}
      [
        height=\plotHeight,
        x=0.5cm,
        xtick={1,...,4},
        xticklabel style={rotate=90},
        bar width=0.3cm,
        enlarge x limits={abs=0.45cm},
        xticklabels={\legBase, \legSlangPlug, \legSlang, \legRTL},
        font={\footnotesize},
	ybar=10pt,
	ymin=0
        ]
\addplot[
    fill=black!25,
    draw=black,
    point meta=y,
    every node near coord/.style={inner ysep=5pt},
    error bars/.cd,
        y dir=both,
        y explicit
]
table [y error=error] {
x   y           error
1   1           0
2   23.2087956   0.1788766172
3   12.9392674   0.017734225
4   24.880755  0.198267254
};
      \end{axis}
    \end{tikzpicture}

   \end{subfigure}
   \subcaption*{\textbf{\LLVMMac}\vspace{-0.2cm}} %change vspace based on fig position
    \end{subfigure}
   \caption{String comparison benchmarks for strings of different sizes on different architectures. Average relative speed-up regarding baseline. The higher the better.}
   \label{fig:bench}
\end{figure*}
\begin{table*}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
    & & 0 & 1 & 5 & 100 & 1000 \\
\hline

  \multirow{2}{*}{\legSlangPlug} &\GCCLinux& 0.413 $\pm$ 0.0005 & 0.746 $\pm$ 0.0008 & 1.804 $\pm$ 0.0005 & 12.05 $\pm$ 0.0088 & 25.36 $\pm$ 0.1742 \\
  						&\LLVMMac& 0.339 $\pm$ 0.0007 & 0.624 $\pm$ 0.0012 & 1.581 $\pm$ 0.0092 & 11.41 $\pm$ 0.0474  & 23.21 $\pm$ 0.1789 \\
   \hline
  \multirow{2}{*}{\legSlang} &\GCCLinux& 1.287 $\pm$ 0.0048 & 2.257 $\pm$ 0.0116 & 4.958 $\pm$ 0.0111 & 17.24 $\pm$ 0.0515 & 25.43 $\pm$ 0.1621\\
 					 &\LLVMMac& 0.932 $\pm$ 0.0015 & 1.690 $\pm$ 0.0030 & 3.241 $\pm$ 0.0067 & 10.15 $\pm$ 0.0321  & 12.94 $\pm$ 0.0177 \\
   \hline
  \multirow{2}{*}{\legRTL}  &\GCCLinux& 3.343 $\pm$ 0.0211 & 5.171 $\pm$ 0.0265  & 8.715 $\pm$ 0.0221 & 17.66 $\pm$ 0.0481 & 26.76 $\pm$ 0.1861\\
 				 	&\LLVMMac & 3.255 $\pm$ 0.0079 & 5.216 $\pm$ 0.0153 & 8.971 $\pm$ 0.0359 & 17.64 $\pm$ 0.06  & 24.88 $\pm$ 0.1983 \\
  \hline
\end{tabular} 
\vspace{0.2cm}
\caption{Benchmark results with standard errors. Average relative speed-up regarding baseline.}
\label{tab:detail_bench}
\end{table*}

\begin{figure*}[b!]
		\centering
		\makebox[0pt]{
		\includegraphics[width=\linewidth]{figures/asm_cmp.pdf}}
		\caption{\GCCLinux and \LLVMMac: different optimizations on the comparison loop.}
		\label{fig:Asm}
\end{figure*}

\subsection{Methodology}

We assessed the execution time of the different implementations by running the following benchmark:

\begin{code}
#(0 1 5 100 1000) collect: [ :size |
	| iterations time overhead |
	collection := ByteString new: size.
	collection2 := ByteString new: size.
	size < 100 
		ifTrue: [iterations := (100000000 // (size*10 max: 1) sqrt) floor.]
		ifFalse: [iterations := (10000000 // size sqrt) floor.].
	overhead := [ 1 to: iterations do: [ :i | ] ] timeToRun.
	time := [ 1 to: iterations do: [ :i |
		collection primitiveStringCompare: collection2 ] ] timeToRun.
	stream nextPutAll: ((time - overhead) asString)].
\end{code}\\

Each benchmark was measured 30 times. Each time, we run a string comparison in a loop and measure the total execution time.

The number of iterations of the loop depends on the string size: the comparison is repeated between 1414 and 10000 times for a short string (5 characters or less), and between 100 and 316 times for a long string (100 characters or more). The warm-up time of the JIT is in this context not noticeable on the performance results: the primitive is compiled to native code usually at the second call as explained in Section \ref{sec:VMexec} and the VM sampling profiler \cite{CogProfiler} confirms no significant time is spent in the interpreter loop (Between 0 and 1 sample over dozens of thousands are shown in the interpreter loop). The number of iterations allows the benchmarks to run for at least 1 second in the slower implementations to avoid processor and OS noises. 

The loop overhead is measured and removed from the measurements (We made sure the JIT did not remove the dead loop to measure it). The two compared strings are equal as this situation corresponds to the worst scenario for most implementations.
\todo{explain how the relative speed up is computed} %There is one less branch in the two plugin implementations in this case.

\subsection{Results} 
We performed the benchmarks upon five different string sizes: 0, 1, 5, 100 and 1000 characters. In the case of small strings, most of the time is spent activating the primitive and in the code before the main comparison loop, the primitive is performed with a little overall number of native instructions. In the case of large strings, the time spent executing the primitive is dominated by the main comparison loop. Figure~\ref{fig:bench} and Table \ref{tab:detail_bench} show the results in terms of relative speed-up towards the baseline implementation, respectively for \GCCLinux and \LLVMMac.

%\cba{Is next paragraph good ?}
%We can draw the following patterns from these results:
%\begin{itemize}
%\item The \legSlangPlug implementation is slower than the baseline version for small strings (0,58 times slower for empty strings, 0,23 times slower for 1-character strings). This is due to the overhead of the calls to the main VM to access the object headers and the stack. On large strings, \legSlangPlug is one of the fastest implementation. 
%\item The \legRTL implementation is more efficient than the baseline in every case. Nevertheless, the performance gap between this version and the \legSlangPlug and \legSlang ones tends to decrease as the string size increases, especially for \GCCLinux. For 1000-characters strings, the \legRTL version goes 0,25 times faster than the \legSlangPlug and the \legSlang versions.
%\item The set-up used (either \GCCLinux or \LLVMMac) does lead to significant differences in terms of performance for the Slang implementation. 
%\end{itemize}

These results can be explained based on the last 3 criteria detailed in Section~\ref{sec:implem_cmp}.

\paragraph{Stack switch overhead.}
As mentioned in Section~\ref{sec:VMexec}, all implementations written in Slang come with an overhead due to the switch between the C and Smalltalk stacks. This overhead is clearly visible in the bench results in Figure~\ref{fig:bench} for small strings (size 1 or less): the Plugin implementation is between 58,7 (\GCCLinux) to 66,1\% (\LLVMMac) slower than the baseline's for empty strings.The problem is even more important for the \legSlangPlug version, since the beginning of its code, before the main comparison loop, is slower due to the calls to the main VM code.
As the string size goes larger, the overhead is less significant because it is absorbed by other performance gains.
Regarding this criteria, \legRTL version is always the fastest as it is not impacted by the switch overhead.

\paragraph{Optimization potential.}
When it comes to comparing large strings (100 characters and more), the execution time is mainly spent in the comparison loop (Figure~\ref{fig:Asm}). The stack switch overhead is still noticeable in the \legSlang implementation when strings of 100 characters are compared: in \GCCLinux, Slang is 17.2 times faster than Baseline while \legRTL is 18,6 times faster than Baseline. Due to the beginning of its code calling the VM code, the \legSlangPlug version is still around 0.68 times slower than the \legSlang and \legRTL version for a string of size 100 (Still in \GCCLinux). For strings of size 1000, the performance of all primitives is very similar, since they have almost the same comparison loop, except for the \legSlang version on \LLVMMac, as we are about to discuss in the next paragraph.

\paragraph{Control over generated code.} One can notice a rather striking performance difference between the 2 set-ups we have. In \GCCLinux, the performance between the \legSlang and \legRTL implementations is really close for long strings (1000 characters wide), with respectively 25.24 and 26.76 relative speed-up factors for a 1000-characters string. Yet performance greatly differs when the \LLVMMac set-up is used, with respectively 12,94 and 24,88 relative speed-up factors.

This difference is mainly due to the optimizations performed or not by the GCC and Clang compilers for the \legSlang version. As depicted in Figure~\ref{fig:Asm}, \GCCLinux generates instructions that are very close to the ones generated by the JIT, hence the execution time is similar. On the contrary, \LLVMMac generates more instructions: it fails at keeping all the operands live across the comparison loops, leading to two extra memory reads per iteration (bolded in Figure \ref{fig:Asm}). 

This is an example of one native function in the whole VM and we do not conclude anything about the C compiler capabilites from it. VMs compiled with LLVM and GCC are around equally fast except in narrow cases like this one and LLVM is able to optimize correctly the \legSlangPlug version. LLVM likely fails at optimizing the \legSlang version due to the old GNU C extensions we use in the interpreter code present in the same file to force variables into specific registers, which are better supported by GCC, the only compiler we supported back in the days. 

However, we can see that the \legRTL version has the most reliable performance since it does not depend on what optimization the C compiler is able to perform or not. This is even more relevant in less common back-ends not shown in this evaluation, such as the MIPSEL back-end we support. On such uncommon processors, C compilers are usually less clever at performing back-end specific optimizations since less engineering time has been invested into it. Cog's RTL is closer to the native instructions and can therefore often generate better code in this case.

\subsection*{Conclusion}In this section, we compared the performance of the different implementations against a baseline (\emph{i.e., }{pure Smalltalk code}) implementation. The results showed the \legRTL implementation was always the fastest. Otherwise, most of the performance gaps between the primitive versions could be explained by 3 criteria: stack switching, optimization for large strings, and the control over generated code.

%We can draw the following patterns from these results:
%\begin{itemize}
%\item The execution time of the SmartSyntax and Plugin implementations is either very close or greater than the baseline's for the small strings. This can be explained by the high cost of the switching between the C and Smalltalk stacks, as mentionned in Section \ref{sec:VMexec}.
%\item The Slang+JIT implementation is more efficient in every case. Nevertheless,
%the performance gap between this implementation and the PureSlang one tends to decrease as the string size increase. The comparison loop (\ref{fig:Asm}) proves to be more and more time-consuming because of the higher number of iterations.
%\end{itemize}
%
%\todo{Following comments are the most important for paper}
%
%\cba{we had 3 criterias:
%- 1 C ST stack switch overhead. 
%- 2 optimization potential, that we may change to large string cmp perf
%- 3 control over generated code
%We need to explain the results based on each criteria, ideally in 3 paragraphs entitled with the 3 criteria.
%The 2 paragraph below explain only point 3 and lacks a conclusion sentence "blabla we have control and if C compiler is not good such as uncommon back-end or x86 which is still quite common in this case..."
%}
%\cba{This is the kind of missing things: C ST stack switch overhead: small string, overhead stack switch high, baseline and JIT version fastest. 
%Large string perf: Large string, loop is where the time is spent, slang and JIT fastest.
%We need size 1000 so that the stack switch overhead cannot be seen anymore on gcc (size 100 we still C it)}
%
%\cba{Maybe small discussion on relevance
%Size 0 is only '' = '', not really interesting in real app, but show well the stack switch overhead.
%Example use cases are comparing tokens in parsers (Size ~4-10 [self, super, thisContext, this, etc.])
%Other example are long urls, hundreds of characters, which are represented with large string comp bench
%or something like that.}
%
%However, one could notice a rather striking performance difference between the 2 compilers: when compiling with GCC, we notive in Figure \ref{fig:bench} that the performances between the PureSlang and Slang+JIT implementations are really close for long strings (100 characters and longer), with respectively 24,42 and 24,48 relative speed-up factors for a 1000-characters string. Yet they greatly differ when LLVM is used, with respectively 13,44 and 26,40 relative speed-up factors.
%
%This difference is mainly due to the optimizations performed by both the compilers: as depicted in Figure \ref{fig:Asm} GCC generates instructions that are very close to the ones generated by the JIT, hence the similar speed-up factors. On the contrary, LLVM generates more instructions: it especially fails at perserving the registries holding the strings (edi and esi), leading to 2 extra instructions (bolded in the figure) that will load the strings at each iteration.

\section{Discussion and related work}
\todo{primitives in other system. (generalize more after Rpython. }

In some programming languages (Strongtalk, Self, RSque-ak), the implementors tried to decrease the overall number of optional primitives by improving the client language performance, reducing the need for such primitives \cite{Strongtalk, ursPHD, NoPrimTracing}. In other work \cite{Ball86a, Char13a}, optional primitives can be written directly in a subset of the client language and compiled at runtime to native code, hence they can be modified in the client language without recompiling the VM. Lastly, several VMs implemented their interpreter on top of an abstract assembly~\cite{V8,LLIntJSC}, allowing the interpreter to use the same register conventions as the code generated by the JIT and avoiding the stack switch cost.

\subsection{Decreasing the number of primitives}

The philosophy of Self and Strongtalk, both high performance VMs, were to improve the performance of the client language with a mature optimizing JIT featuring advanced optimizations such as speculative optimizations \cite{Strongtalk, ursPHD}. In this context, the need of primitives is reduced, since the performance gain from using a primitive over the client language is low or inexistant. More recently, Felgentreff \emph{et al.} \cite{NoPrimTracing} analysed the speed difference between primitives implemented in C, RPython, and Smalltalk. They showed that with the RPython toolchain framework, the speed difference between all three implementations is not that big, hence they could choose to implement some optional primitives in the client language over C or RPython without performance drop to reduce the number of primitives to maintain. \todo{emphasize the difference more}

These approaches have the advantage of decreasing the primitive maintenance cost since fewer primitives have to be maintained. The performance problem is however partially solved: only the peak performance is similar with and without the primitives. Since those systems rely on JITs, start-up performance is worse than peak performance, increasing the performance difference at start-up between the client code and the primitive. In addition, the performance can be  unreliable with optimizing JITs. The implementors of Morphic \cite{MorphicSelf} on top of the Self VM were complaining that changing a single line of code could lead to important performance drop and they would not understand why. Those performance drops were due to the optimizing JIT taking different optimization decisions. 
%This problem was so significant that recently the V8 team \cite{V8} rewrote their primitives written in Javascript (named built-ins in their project) in a DSL compiled ahead of time through their JIT back-end to improve the performance of the primitives and therefore decrease the performance drop between baseline and peak performance. 
Aside from the start-up performance and the performance unreliability problems described, these approaches also require a mature optimizing JIT, hard to implement and maintain.

\subsection{Primitives written in the client language}

In QuickTalk \cite{Ball86a} and Waterfall \cite{Char13a}, the implementors are able to write the primitives in a subset of the client language. The primitives are not part of the VM anymore, but are instead generated as part of the compiled code bytecode representation with a special encoding. They are executed by generating at runtime native code for the special primitive encoding. They can be modified without recompiling the VM and can be performed by the client language implementors instead of the VM implementors, lowering the maintenance cost for the VM implementor. 

However, this design has some issues that can lead to performance drops. Many VMs support multiple processor back-ends and multiple memory representation of objects. Primitives written in such DSL are required to be independent from the processor used and the memory representation of objects. If they were written in the VM, a different version could exist for each different processor and memory representation and a different version would be picked at compile-time. Having different versions allows to refine carefully the instructions generated for performance critical primitives (Array accesses for example) and can lead to noticeable speed-ups.

\subsection{No stack switch overhead}

To avoid paying the stack switch cost between the client Stack and the C stack, some VMs include an interpreter written on top of an abstract assembly. It allows the interpreter to have the same register convention as the native code generated by the JIT, removing the stack switch overhead. For example, the V8 team recently wrote an interpreter named \emph{Ignition}. This interpreter is compiled ahead of time through their JIT back-end to native code. With their primitives and their interpreter written in such way, the V8 runtime rarely needs to switch between the client and the C stack (no need to switch to interpret code, no need to switch for primitives). In Javascript Core, a similar approach is taken with the interpreter named \emph{LLInt}~\cite{LLIntJSC}. LLInt is written in an abstract assembly code compiled ahead of time to native code compatible with the JIT's calling conventions.

In our context, implementing a similar approach would require us to compile Slang to native code through our JIT compiler back-end or to rewrite the interpreter on top of Cog JIT's RTL. Slang has some abstractions over native code, similarly to C. Compiling Slang ahead of time to native code efficiently requires us to re-implement optimizations implemented in C compilers and we don't want that complexity. Alternatively, we could try to lower the abstractions level of Slang to express native code optimizations, but that would increase the complexity of Slang, which we don't want either. Rewriting the interpreter on top of Cog JIT's RTL is possible but requires a significant amount of work. It would be nice to do so as a future work.

\section{Future work and conclusion}

In this section we discuss two future work directions and conclude.

\paragraph{Future work: back-end specific.}

All the primitive implementations we compared are written in a processor independent way, due to the C compiler abstraction for the Slang versions, the abstract assembly abstraction for Cog's RTL version or the VM abstraction for the baseline implementation in Smalltalk. We could rewrite the primitive differently for each back-end we currently support in production (x86, x64, MIPSEL, ARMv6) and for the back-end yet to support (ARMv8, ...). It could be relevant in some cases performance-wise, for example, on Intel processors, the JVM implements string comparison using the SSE4.2 string comparison instructions when available~\cite{JVMSSE4StringComp}. We did not go in this direction because the maintenance cost is too high since we would have to maintain a different implementation per back-end.

\paragraph{Future work: adaptive optimizer integration.}

For the past few years, an experimental adaptive optimizer (a JIT featuring advanced optimizations such as speculative inlining) has been developed on top of the Cog VM \cite{SistaFastStartUp,Bera17a}. The optimizer has specific rules to be able to inline primitive operations without performance loss. We need to carefully measure, in different benchmarks, the performance of the inlined string comparison primitive. We also need to discuss the maintenance cost, for example, do we need a new representation of the primitive for the optimizing JIT to inline it? The micro-benchmarks used in the paper were relevant to show the client stack to C stack switch overhead as well as baseline performance. In the optimizing JIT context, such micro-benchmarks are not really relevant anymore since the optimizing JIT usually entirely optimizes away such benchmarks and real-application performance depends on how well the primitive is inlined and specialized using information from the optimized method (typically, if one operand of the string comparison is a constant string, can the generated code benefit from this knowledge?).

\paragraph{Conclusion.}

In this paper we compared four different implementations of the string comparison primitive on top of the Cog VM. We showed the performance difference between each implementation for different string sizes. Choosing the right implementation for one's VM is a trade-off between maintenance cost and execution time, the quickest to execute the primitive is, the harder it is to maintain.

%\appendix
%\section{Appendix Title}
%
%This is the text of the appendix, if you need one.

\acks
\todo{add acknowledgments}
Acknowledgments, if needed.

% The 'abbrvnat' bibliography style is recommended.
%\bibliographystyle{alpha}
\bibliographystyle{abbrvnat}
\bibliography{sista,rmod,others}

%
%% The bibliography should be embedded for final submission.
%
%\begin{thebibliography}{}
%\softraggedright
%
%\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
%P. Q. Smith, and X. Y. Jones. ...reference text...
%
%\end{thebibliography}


\end{document}
